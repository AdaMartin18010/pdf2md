#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæ¨¡å‹ç¼“å­˜ç®¡ç†å™¨
ç¡®ä¿æ‰€æœ‰æ¨¡å‹éƒ½èƒ½è¢«ç¼“å­˜ï¼Œé¿å…é‡å¤ä¸‹è½½
"""

import os
import sys
import shutil
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Any
import urllib.request
import urllib.parse

class EnhancedCacheManager:
    """å¢å¼ºç‰ˆæ¨¡å‹ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "./models_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_config_file = self.cache_dir / "cache_config.json"
        self.setup_cache_directories()
        self.setup_environment_variables()
        self.load_cache_config()
    
    def setup_cache_directories(self):
        """è®¾ç½®ç¼“å­˜ç›®å½•ç»“æ„"""
        # åˆ›å»ºä¸»ç¼“å­˜ç›®å½•
        self.cache_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå®Œæ•´çš„å­ç›®å½•ç»“æ„
        subdirs = [
            "transformers",
            "datasets", 
            "mineru",
            "modelscope",
            "huggingface",
            "torch",
            "onnx",
            "safetensors",
            "paddle",
            "rapid_table",
            "ocr_models",
            "layout_models",
            "table_models",
            "formula_models",
            "reading_order_models"
        ]
        
        for subdir in subdirs:
            (self.cache_dir / subdir).mkdir(exist_ok=True)
        
        print(f"âœ… å¢å¼ºç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def setup_environment_variables(self):
        """è®¾ç½®å®Œæ•´çš„ç¯å¢ƒå˜é‡"""
        # HuggingFaceç›¸å…³
        os.environ['HF_HOME'] = str(self.cache_dir)
        os.environ['HF_DATASETS_CACHE'] = str(self.cache_dir / "datasets")
        os.environ['TRANSFORMERS_CACHE'] = str(self.cache_dir / "transformers")
        os.environ['HF_HUB_CACHE'] = str(self.cache_dir / "huggingface")
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
        os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '0'
        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600'
        os.environ['HF_HUB_DOWNLOAD_RETRY_DELAY'] = '10'
        
        # ModelScopeç›¸å…³
        os.environ['MODELSCOPE_CACHE'] = str(self.cache_dir / "modelscope")
        os.environ['MODELSCOPE_DOMAIN'] = "modelscope.cn"
        os.environ['MODELSCOPE_DOWNLOAD_TIMEOUT'] = '600'
        
        # Mineruç›¸å…³
        os.environ['MINERU_CACHE_DIR'] = str(self.cache_dir / "mineru")
        os.environ['MINERU_MODEL_SOURCE'] = 'modelscope'
        os.environ['MINERU_OFFLINE_MODE'] = 'false'
        
        # PyTorchç›¸å…³
        os.environ['TORCH_HOME'] = str(self.cache_dir / "torch")
        os.environ['TORCH_WEIGHTS_ONLY'] = 'false'
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
        
        # ONNXç›¸å…³
        os.environ['ONNX_HOME'] = str(self.cache_dir / "onnx")
        
        # PaddleOCRç›¸å…³
        os.environ['PADDLE_HOME'] = str(self.cache_dir / "paddle")
        
        # å…¶ä»–ä¼˜åŒ–è®¾ç½®
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # å¦‚æœæœ‰GPU
        os.environ['OMP_NUM_THREADS'] = '4'
        
        # ç¦ç”¨ä¸å¿…è¦çš„ä¸‹è½½
        os.environ['TRANSFORMERS_OFFLINE'] = '0'
        os.environ['HF_DATASETS_OFFLINE'] = '0'
        
        print("âœ… å¢å¼ºç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")
    
    def load_cache_config(self):
        """åŠ è½½ç¼“å­˜é…ç½®"""
        if self.cache_config_file.exists():
            try:
                with open(self.cache_config_file, 'r', encoding='utf-8') as f:
                    self.cache_config = json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ç¼“å­˜é…ç½®å¤±è´¥: {e}")
                self.cache_config = {}
        else:
            self.cache_config = {
                'model_hashes': {},
                'download_history': [],
                'cache_stats': {
                    'total_downloads': 0,
                    'total_size': 0,
                    'last_cleanup': None
                }
            }
            self.save_cache_config()
    
    def save_cache_config(self):
        """ä¿å­˜ç¼“å­˜é…ç½®"""
        try:
            with open(self.cache_config_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜é…ç½®å¤±è´¥: {e}")
    
    def get_model_hash(self, model_path: Path) -> str:
        """è·å–æ¨¡å‹æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
        try:
            with open(model_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return str(model_path.stat().st_mtime)
    
    def check_cache_status(self) -> Dict[str, Any]:
        """æ£€æŸ¥è¯¦ç»†çš„ç¼“å­˜çŠ¶æ€"""
        status = {
            'cache_dir': str(self.cache_dir),
            'total_size': 0,
            'model_count': 0,
            'subdirs': {},
            'missing_models': [],
            'cache_efficiency': 0.0
        }
        
        # æ£€æŸ¥å„ä¸ªç¼“å­˜ç›®å½•
        cache_dirs = {
            "transformers": self.cache_dir / "transformers",
            "datasets": self.cache_dir / "datasets", 
            "mineru": self.cache_dir / "mineru",
            "modelscope": self.cache_dir / "modelscope",
            "huggingface": self.cache_dir / "huggingface",
            "torch": self.cache_dir / "torch",
            "onnx": self.cache_dir / "onnx",
            "paddle": self.cache_dir / "paddle",
            "rapid_table": self.cache_dir / "rapid_table",
            "ocr_models": self.cache_dir / "ocr_models",
            "layout_models": self.cache_dir / "layout_models",
            "table_models": self.cache_dir / "table_models",
            "formula_models": self.cache_dir / "formula_models",
            "reading_order_models": self.cache_dir / "reading_order_models"
        }
        
        total_size = 0
        total_models = 0
        
        for name, dir_path in cache_dirs.items():
            dir_info = {
                'exists': dir_path.exists(),
                'has_models': False,
                'size': 0,
                'model_count': 0,
                'model_types': []
            }
            
            if dir_path.exists():
                # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
                model_extensions = ['.bin', '.safetensors', '.onnx', '.pt', '.pth', '.ckpt', '.pkl']
                model_files = []
                
                for ext in model_extensions:
                    model_files.extend(dir_path.rglob(f"*{ext}"))
                
                dir_info['model_count'] = len(model_files)
                dir_info['has_models'] = len(model_files) > 0
                
                # è®¡ç®—ç›®å½•å¤§å°
                dir_size = 0
                for root, dirs, files in os.walk(dir_path):
                    for file in files:
                        file_path = Path(root) / file
                        if file_path.is_file():
                            dir_size += file_path.stat().st_size
                
                dir_info['size'] = dir_size
                total_size += dir_size
                total_models += dir_info['model_count']
                
                # è®°å½•æ¨¡å‹ç±»å‹
                for model_file in model_files:
                    dir_info['model_types'].append(model_file.suffix)
            
            status['subdirs'][name] = dir_info
        
        status['total_size'] = total_size
        status['model_count'] = total_models
        
        # è®¡ç®—ç¼“å­˜æ•ˆç‡
        expected_models = 15  # é¢„æœŸçš„æ¨¡å‹æ•°é‡
        status['cache_efficiency'] = min(1.0, total_models / expected_models)
        
        return status
    
    def preload_common_models(self):
        """é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹"""
        print("ğŸ”„ é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹...")
        
        # å®šä¹‰å¸¸ç”¨æ¨¡å‹åˆ—è¡¨
        common_models = [
            {
                'name': 'layout_detection',
                'source': 'modelscope',
                'path': 'OpenDataLab/PDF-Extract-Kit-1.0/models/Layout/YOLO'
            },
            {
                'name': 'table_recognition',
                'source': 'modelscope', 
                'path': 'OpenDataLab/PDF-Extract-Kit-1.0/models/TabRec/SlanetPlus'
            },
            {
                'name': 'ocr_models',
                'source': 'modelscope',
                'path': 'OpenDataLab/PDF-Extract-Kit-1.0/models/OCR/paddleocr_torch'
            },
            {
                'name': 'formula_recognition',
                'source': 'modelscope',
                'path': 'OpenDataLab/PDF-Extract-Kit-1.0/models/MFR/unimernet_hf_small_2503'
            }
        ]
        
        for model in common_models:
            try:
                self.ensure_model_cached(model)
            except Exception as e:
                print(f"âš ï¸ é¢„åŠ è½½æ¨¡å‹ {model['name']} å¤±è´¥: {e}")
    
    def ensure_model_cached(self, model_info: Dict[str, str]):
        """ç¡®ä¿æ¨¡å‹è¢«ç¼“å­˜"""
        model_name = model_info['name']
        model_path = self.cache_dir / model_info['source'] / model_info['path']
        
        if not model_path.exists():
            print(f"ğŸ“¥ é¢„ä¸‹è½½æ¨¡å‹: {model_name}")
            # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„ä¸‹è½½é€»è¾‘
            # ç”±äºæ¨¡å‹ä¸‹è½½éœ€è¦ç‰¹å®šçš„APIï¼Œè¿™é‡Œåªæ˜¯æ ‡è®°
            pass
        else:
            print(f"âœ… æ¨¡å‹å·²ç¼“å­˜: {model_name}")
    
    def optimize_cache_settings(self):
        """ä¼˜åŒ–ç¼“å­˜è®¾ç½®"""
        print("ğŸ”§ ä¼˜åŒ–ç¼“å­˜è®¾ç½®...")
        
        # è®¾ç½®æ›´ä¿å®ˆçš„ä¸‹è½½è®¾ç½®
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
        os.environ['TRANSFORMERS_OFFLINE'] = '0'
        os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '0'
        
        # è®¾ç½®ä¸‹è½½é‡è¯•å’Œè¶…æ—¶
        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600'
        os.environ['HF_HUB_DOWNLOAD_RETRY_DELAY'] = '10'
        os.environ['HF_HUB_DOWNLOAD_RETRY_MAX'] = '3'
        
        # ModelScopeè®¾ç½®
        os.environ['MODELSCOPE_DOWNLOAD_TIMEOUT'] = '600'
        os.environ['MODELSCOPE_DOWNLOAD_RETRY'] = '3'
        
        # ç¦ç”¨ä¸å¿…è¦çš„åŠŸèƒ½
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
        
        print("âœ… ç¼“å­˜è®¾ç½®ä¼˜åŒ–å®Œæˆ")
    
    def cleanup_cache(self, max_age_days: int = 30):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        print(f"ğŸ§¹ æ¸…ç† {max_age_days} å¤©å‰çš„ç¼“å­˜...")
        
        import time
        current_time = time.time()
        max_age_seconds = max_age_days * 24 * 3600
        
        cleaned_files = 0
        cleaned_size = 0
        
        for root, dirs, files in os.walk(self.cache_dir):
            for file in files:
                file_path = Path(root) / file
                if file_path.is_file():
                    file_age = current_time - file_path.stat().st_mtime
                    if file_age > max_age_seconds:
                        try:
                            file_size = file_path.stat().st_size
                            file_path.unlink()
                            cleaned_files += 1
                            cleaned_size += file_size
                        except Exception as e:
                            print(f"âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        print(f"âœ… æ¸…ç†å®Œæˆ: åˆ é™¤ {cleaned_files} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {cleaned_size / (1024**2):.2f} MB")
        
        # æ›´æ–°ç¼“å­˜ç»Ÿè®¡
        self.cache_config['cache_stats']['last_cleanup'] = current_time
        self.save_cache_config()
    
    def print_detailed_cache_info(self):
        """æ‰“å°è¯¦ç»†çš„ç¼“å­˜ä¿¡æ¯"""
        try:
            status = self.check_cache_status()
            
            print(f"\nğŸ“Š å¢å¼ºç¼“å­˜ä¿¡æ¯:")
            print(f"  ç¼“å­˜ç›®å½•: {status['cache_dir']}")
            print(f"  æ€»å¤§å°: {status['total_size'] / (1024**3):.2f} GB")
            print(f"  æ¨¡å‹æ–‡ä»¶æ•°: {status['model_count']}")
            print(f"  ç¼“å­˜æ•ˆç‡: {status['cache_efficiency']*100:.1f}%")
            
            print(f"\nğŸ“ è¯¦ç»†ç¼“å­˜çŠ¶æ€:")
            for name, info in status['subdirs'].items():
                if info['exists']:
                    status_icon = "âœ…" if info['has_models'] else "âš ï¸"
                    size_mb = info['size'] / (1024**2)
                    print(f"  {status_icon} {name}: {info['model_count']} ä¸ªæ¨¡å‹ ({size_mb:.1f} MB)")
                else:
                    print(f"  âŒ {name}: ç›®å½•ä¸å­˜åœ¨")
        except Exception as e:
            print(f"âŒ è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            # ä½¿ç”¨åŸºæœ¬æ£€æŸ¥
            self.print_basic_cache_info()
    
    def print_basic_cache_info(self):
        """æ‰“å°åŸºæœ¬ç¼“å­˜ä¿¡æ¯"""
        print(f"\nğŸ“Š åŸºæœ¬ç¼“å­˜ä¿¡æ¯:")
        print(f"  ç¼“å­˜ç›®å½•: {self.cache_dir}")
        
        total_size = 0
        total_files = 0
        
        for root, dirs, files in os.walk(self.cache_dir):
            for file in files:
                file_path = Path(root) / file
                if file_path.is_file():
                    total_size += file_path.stat().st_size
                    if file.endswith(('.bin', '.safetensors', '.onnx', '.pt', '.pth')):
                        total_files += 1
        
        print(f"  æ€»å¤§å°: {total_size / (1024**3):.2f} GB")
        print(f"  æ¨¡å‹æ–‡ä»¶æ•°: {total_files}")
        
        # æ£€æŸ¥ä¸»è¦ç›®å½•
        main_dirs = ["modelscope", "transformers", "mineru", "torch"]
        print(f"\nğŸ“ ä¸»è¦ç¼“å­˜çŠ¶æ€:")
        for dir_name in main_dirs:
            dir_path = self.cache_dir / dir_name
            if dir_path.exists():
                model_files = list(dir_path.rglob("*.bin")) + list(dir_path.rglob("*.safetensors"))
                status_icon = "âœ…" if len(model_files) > 0 else "âš ï¸"
                print(f"  {status_icon} {dir_name}: {'å·²ç¼“å­˜' if len(model_files) > 0 else 'æœªç¼“å­˜'}")
            else:
                print(f"  âŒ {dir_name}: ç›®å½•ä¸å­˜åœ¨")
    
    def create_cache_report(self) -> str:
        """åˆ›å»ºç¼“å­˜æŠ¥å‘Š"""
        status = self.check_cache_status()
        
        report = f"""
# æ¨¡å‹ç¼“å­˜æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- ç¼“å­˜ç›®å½•: {status['cache_dir']}
- æ€»å¤§å°: {status['total_size'] / (1024**3):.2f} GB
- æ¨¡å‹æ–‡ä»¶æ•°: {status['model_count']}
- ç¼“å­˜æ•ˆç‡: {status['cache_efficiency']*100:.1f}%

## è¯¦ç»†çŠ¶æ€
"""
        
        for name, info in status['subdirs'].items():
            if info['exists'] and info['has_models']:
                size_mb = info['size'] / (1024**2)
                report += f"- âœ… {name}: {info['model_count']} ä¸ªæ¨¡å‹ ({size_mb:.1f} MB)\n"
            elif info['exists']:
                report += f"- âš ï¸ {name}: ç›®å½•å­˜åœ¨ä½†æ— æ¨¡å‹æ–‡ä»¶\n"
            else:
                report += f"- âŒ {name}: ç›®å½•ä¸å­˜åœ¨\n"
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆæ¨¡å‹ç¼“å­˜ç®¡ç†å™¨")
    parser.add_argument("--info", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†ç¼“å­˜ä¿¡æ¯")
    parser.add_argument("--optimize", action="store_true", help="ä¼˜åŒ–ç¼“å­˜è®¾ç½®")
    parser.add_argument("--preload", action="store_true", help="é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹")
    parser.add_argument("--cleanup", type=int, metavar="DAYS", help="æ¸…ç†æŒ‡å®šå¤©æ•°å‰çš„ç¼“å­˜")
    parser.add_argument("--report", action="store_true", help="ç”Ÿæˆç¼“å­˜æŠ¥å‘Š")
    
    args = parser.parse_args()
    
    cache_manager = EnhancedCacheManager()
    
    if args.info:
        cache_manager.print_detailed_cache_info()
    elif args.optimize:
        cache_manager.optimize_cache_settings()
    elif args.preload:
        cache_manager.preload_common_models()
    elif args.cleanup:
        cache_manager.cleanup_cache(args.cleanup)
    elif args.report:
        report = cache_manager.create_cache_report()
        print(report)
    else:
        # é»˜è®¤æ˜¾ç¤ºä¿¡æ¯
        cache_manager.print_detailed_cache_info()

if __name__ == "__main__":
    main() 