#!/usr/bin/env python3
"""
å†…å­˜å’ŒGPUç›‘æ§è„šæœ¬
"""

import time
import psutil
import os
import sys

def get_memory_info():
    """è·å–å†…å­˜ä¿¡æ¯"""
    try:
        memory = psutil.virtual_memory()
        return {
            'total': memory.total / (1024**3),  # GB
            'available': memory.available / (1024**3),  # GB
            'used': memory.used / (1024**3),  # GB
            'percent': memory.percent
        }
    except:
        return None

def get_gpu_info():
    """è·å–GPUä¿¡æ¯"""
    try:
        import torch
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            cached = torch.cuda.memory_reserved(0) / (1024**3)
            return {
                'available': gpu_memory,
                'allocated': allocated,
                'cached': cached,
                'free': gpu_memory - allocated
            }
        else:
            return None
    except:
        return None

def monitor_resources(duration=60, interval=2):
    """ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ"""
    print(f"ğŸ” å¼€å§‹ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ ({duration}ç§’)...")
    print("=" * 60)
    
    start_time = time.time()
    max_memory = 0
    max_gpu = 0
    
    while time.time() - start_time < duration:
        # è·å–å†…å­˜ä¿¡æ¯
        memory_info = get_memory_info()
        if memory_info:
            print(f"ğŸ’¾ å†…å­˜: {memory_info['used']:.1f}GB / {memory_info['total']:.1f}GB ({memory_info['percent']:.1f}%)")
            max_memory = max(max_memory, memory_info['used'])
        
        # è·å–GPUä¿¡æ¯
        gpu_info = get_gpu_info()
        if gpu_info:
            print(f"ğŸ® GPU: {gpu_info['allocated']:.1f}GB / {gpu_info['available']:.1f}GB (ç¼“å­˜: {gpu_info['cached']:.1f}GB)")
            max_gpu = max(max_gpu, gpu_info['allocated'])
        else:
            print("ğŸ® GPU: ä¸å¯ç”¨")
        
        # è·å–è¿›ç¨‹ä¿¡æ¯
        try:
            process = psutil.Process()
            process_memory = process.memory_info().rss / (1024**2)  # MB
            print(f"ğŸ“Š å½“å‰è¿›ç¨‹: {process_memory:.1f}MB")
        except:
            print("ğŸ“Š å½“å‰è¿›ç¨‹: æœªçŸ¥")
        
        print("-" * 40)
        time.sleep(interval)
    
    print("=" * 60)
    print(f"ğŸ“ˆ ç›‘æ§å®Œæˆ!")
    print(f"ğŸ’¾ æœ€å¤§å†…å­˜ä½¿ç”¨: {max_memory:.1f}GB")
    if max_gpu > 0:
        print(f"ğŸ® æœ€å¤§GPUä½¿ç”¨: {max_gpu:.1f}GB")

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½å¯¹å†…å­˜çš„å½±å“"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½å¯¹å†…å­˜çš„å½±å“...")
    
    # è®°å½•åˆå§‹çŠ¶æ€
    initial_memory = get_memory_info()
    initial_gpu = get_gpu_info()
    
    print("ğŸ“Š åˆå§‹çŠ¶æ€:")
    if initial_memory:
        print(f"  å†…å­˜: {initial_memory['used']:.1f}GB")
    if initial_gpu:
        print(f"  GPU: {initial_gpu['allocated']:.1f}GB")
    
    try:
        print("\nğŸ”„ å°è¯•å¯¼å…¥ç¼“å­˜ç®¡ç†å™¨...")
        from enhanced_cache_manager import EnhancedCacheManager
        cache_manager = EnhancedCacheManager()
        
        # è®°å½•å¯¼å…¥åçŠ¶æ€
        after_import_memory = get_memory_info()
        after_import_gpu = get_gpu_info()
        
        print("ğŸ“Š å¯¼å…¥ç¼“å­˜ç®¡ç†å™¨å:")
        if after_import_memory and initial_memory:
            memory_diff = after_import_memory['used'] - initial_memory['used']
            print(f"  å†…å­˜å˜åŒ–: {memory_diff:+.1f}GB")
        if after_import_gpu and initial_gpu:
            gpu_diff = after_import_gpu['allocated'] - initial_gpu['allocated']
            print(f"  GPUå˜åŒ–: {gpu_diff:+.1f}GB")
        
        print("\nğŸ”„ å°è¯•æ£€æŸ¥ç¼“å­˜çŠ¶æ€...")
        cache_info = cache_manager.check_cache_status()
        
        # è®°å½•æ£€æŸ¥åçŠ¶æ€
        after_check_memory = get_memory_info()
        after_check_gpu = get_gpu_info()
        
        print("ğŸ“Š æ£€æŸ¥ç¼“å­˜å:")
        if after_check_memory and initial_memory:
            memory_diff = after_check_memory['used'] - initial_memory['used']
            print(f"  å†…å­˜å˜åŒ–: {memory_diff:+.1f}GB")
        if after_check_gpu and initial_gpu:
            gpu_diff = after_check_gpu['allocated'] - initial_gpu['allocated']
            print(f"  GPUå˜åŒ–: {gpu_diff:+.1f}GB")
        
        print("\nâœ… æ¨¡å‹åŠ è½½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å†…å­˜å’ŒGPUç›‘æ§å·¥å…·")
    print("=" * 60)
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "monitor":
            duration = int(sys.argv[2]) if len(sys.argv) > 2 else 60
            monitor_resources(duration)
        elif command == "test":
            test_model_loading()
        else:
            print("ç”¨æ³•:")
            print("  python memory_monitor.py monitor [æŒç»­æ—¶é—´ç§’]")
            print("  python memory_monitor.py test")
    else:
        # é»˜è®¤è¿è¡Œç›‘æ§
        monitor_resources(30)

if __name__ == "__main__":
    main() 