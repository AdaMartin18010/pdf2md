#!/usr/bin/env python3
"""
æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨
ç¡®ä¿æ¨¡å‹åªä¸‹è½½ä¸€æ¬¡ï¼Œé¿å…é‡å¤ä¸‹è½½
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any

class ModelCacheManager:
    """æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "./models_cache"):
        self.cache_dir = Path(cache_dir)
        self.setup_cache_directories()
        self.setup_environment_variables()
    
    def setup_cache_directories(self):
        """è®¾ç½®ç¼“å­˜ç›®å½•ç»“æ„"""
        # åˆ›å»ºä¸»ç¼“å­˜ç›®å½•
        self.cache_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        subdirs = [
            "transformers",
            "datasets", 
            "mineru",
            "modelscope",
            "huggingface",
            "torch",
            "onnx",
            "safetensors"
        ]
        
        for subdir in subdirs:
            (self.cache_dir / subdir).mkdir(exist_ok=True)
        
        print(f"âœ… æ¨¡å‹ç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def setup_environment_variables(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        # HuggingFaceç›¸å…³
        os.environ['HF_HOME'] = str(self.cache_dir)
        os.environ['HF_DATASETS_CACHE'] = str(self.cache_dir / "datasets")
        os.environ['TRANSFORMERS_CACHE'] = str(self.cache_dir / "transformers")
        os.environ['HF_HUB_CACHE'] = str(self.cache_dir / "huggingface")
        
        # ModelScopeç›¸å…³
        os.environ['MODELSCOPE_CACHE'] = str(self.cache_dir / "modelscope")
        os.environ['MODELSCOPE_DOMAIN'] = "modelscope.cn"
        
        # Mineruç›¸å…³
        os.environ['MINERU_CACHE_DIR'] = str(self.cache_dir / "mineru")
        os.environ['MINERU_MODEL_SOURCE'] = 'modelscope'
        
        # PyTorchç›¸å…³
        os.environ['TORCH_HOME'] = str(self.cache_dir / "torch")
        os.environ['TORCH_WEIGHTS_ONLY'] = 'false'
        
        # ONNXç›¸å…³
        os.environ['ONNX_HOME'] = str(self.cache_dir / "onnx")
        
        # å…¶ä»–ä¼˜åŒ–è®¾ç½®
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
        
        print("âœ… ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")
    
    def check_cache_status(self) -> Dict[str, bool]:
        """æ£€æŸ¥ç¼“å­˜çŠ¶æ€"""
        status = {}
        
        # æ£€æŸ¥å„ä¸ªç¼“å­˜ç›®å½•
        cache_dirs = {
            "transformers": self.cache_dir / "transformers",
            "datasets": self.cache_dir / "datasets", 
            "mineru": self.cache_dir / "mineru",
            "modelscope": self.cache_dir / "modelscope",
            "huggingface": self.cache_dir / "huggingface",
            "torch": self.cache_dir / "torch"
        }
        
        for name, dir_path in cache_dirs.items():
            if dir_path.exists():
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶
                model_files = list(dir_path.rglob("*.bin")) + list(dir_path.rglob("*.safetensors")) + list(dir_path.rglob("*.onnx"))
                status[name] = len(model_files) > 0
            else:
                status[name] = False
        
        return status
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        info = {
            "cache_dir": str(self.cache_dir),
            "total_size": 0,
            "model_count": 0,
            "status": self.check_cache_status()
        }
        
        # è®¡ç®—æ€»å¤§å°
        total_size = 0
        model_count = 0
        
        for root, dirs, files in os.walk(self.cache_dir):
            for file in files:
                file_path = Path(root) / file
                if file_path.is_file():
                    total_size += file_path.stat().st_size
                    if file.endswith(('.bin', '.safetensors', '.onnx', '.pt', '.pth')):
                        model_count += 1
        
        info["total_size"] = total_size
        info["model_count"] = model_count
        
        return info
    
    def print_cache_info(self):
        """æ‰“å°ç¼“å­˜ä¿¡æ¯"""
        info = self.get_cache_info()
        status = info["status"]
        
        print(f"\nğŸ“Š æ¨¡å‹ç¼“å­˜ä¿¡æ¯:")
        print(f"  ç¼“å­˜ç›®å½•: {info['cache_dir']}")
        print(f"  æ€»å¤§å°: {info['total_size'] / (1024**3):.2f} GB")
        print(f"  æ¨¡å‹æ–‡ä»¶æ•°: {info['model_count']}")
        
        print(f"\nğŸ“ ç¼“å­˜çŠ¶æ€:")
        for name, has_models in status.items():
            status_icon = "âœ…" if has_models else "âŒ"
            print(f"  {status_icon} {name}: {'å·²ç¼“å­˜' if has_models else 'æœªç¼“å­˜'}")
    
    def clear_cache(self, confirm: bool = False):
        """æ¸…ç©ºç¼“å­˜"""
        if not confirm:
            print("âš ï¸ è¿™å°†åˆ é™¤æ‰€æœ‰ç¼“å­˜çš„æ¨¡å‹æ–‡ä»¶")
            response = input("ç¡®è®¤æ¸…ç©ºç¼“å­˜? (y/N): ").strip().lower()
            if response != 'y':
                print("âŒ å–æ¶ˆæ¸…ç©ºç¼“å­˜")
                return False
        
        try:
            import shutil
            shutil.rmtree(self.cache_dir)
            self.setup_cache_directories()
            print("âœ… ç¼“å­˜å·²æ¸…ç©º")
            return True
        except Exception as e:
            print(f"âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def optimize_for_first_run(self):
        """ä¼˜åŒ–é¦–æ¬¡è¿è¡Œ"""
        print("ğŸ”§ ä¼˜åŒ–é¦–æ¬¡è¿è¡Œè®¾ç½®...")
        
        # è®¾ç½®æ›´ä¿å®ˆçš„ä¸‹è½½è®¾ç½®
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
        os.environ['TRANSFORMERS_OFFLINE'] = '0'
        os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '0'
        
        # è®¾ç½®ä¸‹è½½é‡è¯•
        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'
        os.environ['HF_HUB_DOWNLOAD_RETRY_DELAY'] = '5'
        
        print("âœ… é¦–æ¬¡è¿è¡Œä¼˜åŒ–å®Œæˆ")
        print("ğŸ’¡ é¦–æ¬¡è¿è¡Œå°†ä¸‹è½½å¿…è¦çš„æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨")
    parser.add_argument("--info", action="store_true", help="æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯")
    parser.add_argument("--clear", action="store_true", help="æ¸…ç©ºç¼“å­˜")
    parser.add_argument("--optimize", action="store_true", help="ä¼˜åŒ–é¦–æ¬¡è¿è¡Œ")
    
    args = parser.parse_args()
    
    cache_manager = ModelCacheManager()
    
    if args.info:
        cache_manager.print_cache_info()
    elif args.clear:
        cache_manager.clear_cache()
    elif args.optimize:
        cache_manager.optimize_for_first_run()
    else:
        # é»˜è®¤æ˜¾ç¤ºä¿¡æ¯
        cache_manager.print_cache_info()

if __name__ == "__main__":
    main() 